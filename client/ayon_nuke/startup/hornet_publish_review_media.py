import os
import getpass
import shutil
import nuke
import json
from hornet_deadline_utils import get_deadline_url
from pathlib import Path
from datetime import datetime
from file_sequence import SequenceFactory

try:
    from ayon_core.settings import get_current_project_settings  # type: ignore
    from ayon_api import get_bundle_settings  # type: ignore
    import requests
except ImportError:
    print("failed to import ayon_core or ayon_api. this is probably fine.")

"""
This module works in the following sequence:

- this module is loaded into the user's nuke envuronment via menu.py
- integrate_prores_review.py runs as a standard pyblish plugin
- it calls either hornet_review_media_submit or generate_review_media_local

Farm pipeline:
    - the data dict from integrate_prores_review is dumped into an env variable
    - a template nuke script is copied to a submission directory
    - it is submitted as a deadline render job
    - it has hornet_publish_configurate set as an onScriptLoad callback
    - that function reads the data from the env variable and configures the nodes once the farm loads it
    - it renders the appropriate review media
    - since we cannot use the logger, a log file is created in the submission directory for debugging

Local pipelione:
    - since we are in the same context, env variable is not needed
    - the nodes are pasted from the template script
    - they are configured according to the data dict
    - they are rendered locally
    - tey are deleted


GOTCHA!

watch out! If you rename the module or function called from the onScriptLoad callback,
you will need to update the import statements in the template script.

that one had me going for a while.

"""


COLORSPACE_LOOKUP = {
    "Output - Rec.709": "rec709",
    "Output - sRGB": "sRGB",
    "Output - Rec.2020": "rec2020",
    "ACES - ACEScg": "ACEScg",
    "color_picking": "sRGB",
}

"""
    colorspace lookup to translate the fancy colorspace names
    into something that works as part of a file name

    add more as needed

    if an unlisted colorspace is passed, spaces will be removed
"""


def hornet_review_media_submit(data, logger=None):
    """
    Submit review media renders to deadline.
    These are standard nuke deadline render jobs.

    Args:
        data (dict): Data dictionary containing review media submission information, generated by integrate_prores_review.py
        logger (Logger): Logger object for logging messages

    Returns:
        bool: True if all submissions were successful, False otherwise
    """
    log = MiniLogger(logger)

    env_vars = json.dumps(data)
    nd = nuke.toNode(data["writeNode"])

    fs = SequenceFactory.from_nuke_node(nd)
    if fs is None:
        log.raise_exception("failed to get file sequence")

    log.debug(f"file sequence: {fs}")

    global_first = fs.first_frame
    global_last = fs.last_frame

    validate_template_script(data.get("template_script", None), logger)

    """
    data is dumped into an env var because hornet_publish_configurate is called
    by the onScriptLoad callback, which means we cannot pass any arguments to it.
    """
    write_node_info = discover_write_nodes_in_script(data["template_script"])

    log.debug(f"write_nodes: {write_node_info}")

    deadline_url = get_deadline_url()
    successful_submissions = 0
    failed_submissions = 0

    batch_name = _get_batch_name(data)
    log.info(f"Using batch name: {batch_name}")

    for node_info in write_node_info:
        node_name = node_info[0]
        first = node_info[1]
        last = node_info[2]
        submission_script = resolve_submission_script(
            data,
            write_node_name=node_name,
            logger=logger,
        )

        submission_info = {
            "task_name": f"{data['shot']}_{data['name']}_{node_name}_review",
            "deadlinePriority": 95,
            "deadlinePool": "local",
            "deadlineGroup": "nuke",
            "deadlineChunkSize": last
            - first
            + 1,  # Render entire sequence in one chunk for review media
            "concurrentTasks": 1,
            "Frames": f"{first}-{last}",  # Let Deadline determine frame range from node metadata
            "write_node_name": f"{node_name}",
            "deadline_job_id": data.get(
                "deadline_job_id"
            ),  # Pass deadline job ID
            "job_type": data.get("job_type"),  # Pass job type (render/publish)
            "render_target": data.get("render_target"),  # Pass render target
            "batch": batch_name,
        }

        body = build_request(submission_info, submission_script, env_vars)
        log.debug(f"body for {node_name}: {body}")
        print(f"body: {body}")

        if "requests" not in globals():
            log.raise_exception(
                "requests module not available - needed for deadline submission"
            )

        response = requests.post(deadline_url, json=body, timeout=10)

        if not response.ok:
            failed_submissions += 1
        else:
            successful_submissions += 1

    log.debug(f"successful_submissions: {successful_submissions}")
    log.debug(f"failed_submissions: {failed_submissions}")

    return failed_submissions == 0


def generate_review_media_local(data, logger=None):
    """
    Generate review media locally.
    Pastes nodes into the user's active session, renders, and deletes them.

    Args:
        data (dict): Data dictionary containing review media submission information, generated by integrate_prores_review.py
        logger (Logger): Logger object for logging messages
    """
    log = MiniLogger(logger)

    log.info("generate_review_media_local log")

    if data is None:
        log.raise_exception("data is None")

    log.debug(f"data: {data}")

    # Get review media template script
    template_script = data.get("template_script", None)
    if template_script is None:
        log.raise_exception("template_script is None")


    # Get list of nodes from template script
    for node in nuke.allNodes():
        node.setSelected(False)

    current_nodes = set(nuke.allNodes())
    new_nodes = set()
    nuke.nodePaste(template_script)
    new_nodes = set(nuke.allNodes()) - current_nodes
    
    # TODO set env for local generation
    # Same code as found in the JobLoad script for deadline nuke
    pub_data = data
    new_env = {}

    first_frame_key = "REVIEW_FF"
    first_frame_value = str(pub_data["first_frame"])
    new_env[first_frame_key] = first_frame_value

    last_frame_key = "REVIEW_LF"
    last_frame_val = str(pub_data["last_frame"])
    new_env[last_frame_key] = last_frame_val

    shot_name_key = "REVIEW_SHOT_NAME"
    shot_name_val = pub_data["shot"]
    new_env[shot_name_key] = shot_name_val

    render_name_key = "REVIEW_RENDER_NAME"
    render_name_val = pub_data["name"]
    new_env[render_name_key] = render_name_val

    burnin_key = "REVIEW_BURNIN"
    burnin_val = str(pub_data["burnin"])
    new_env[burnin_key] = burnin_val

    version_key = "REVIEW_VERSION"
    version_val = str(pub_data["version"])
    new_env[version_key] = version_val

    project_name_key = "REVIEW_PROJECT_NAME"
    project_name_val = pub_data["project"]
    new_env[project_name_key] = project_name_val

    pub_seq_key = "REVIEW_PUB_SEQ"
    pub_seq_val = pub_data["publishedSequence"]
    new_env[pub_seq_key] = pub_seq_val

    # Inject environment variables for the nuke script
    for k,v in new_env.items():
        os.environ[k] = v

    # minimise clutter in user's node graph in case the script fails and they have to delete them
    backdrops = []
    for node in new_nodes:
        if node.Class() == "BackdropNode":
            nuke.delete(node)
            backdrops.append(node)
    for backdrop in backdrops:
        new_nodes.remove(backdrop)

    for node in new_nodes:
        node.setSelected(True)

    nuke.autoplace_all()

    # get review write nodes only
    write_nodes = [n.name() for n in new_nodes if n.Class() == "Write"]

    # this is the same script that is called from the onScriptLoad callback on the farm
    # deprecated way to inject review data into template, now uses env variables to enable farm
    #hornet_publish_configurate(data, new_nodes)


    # render the review media - on farm handled by the deadline plugin 
    for node_name in write_nodes:
        n = nuke.toNode(node_name)

        output_path = Path(pub_data["publishDir"]) / "review"
        output_name = pub_data["shot"]+"_"+pub_data["name"]+"_"+node_name
        output_path_key = "REVIEW_"+node_name
        output_path_val = output_path / output_name
        os.environ[output_path_key] = output_path_val.as_posix()
        log.info(f"Set output environment variables {output_path_key} = {output_path_val.as_posix()}")

        if nuke.toNode(node_name) is None:
            log.error(f"failed to get write node: {node_name}")
            print(f"failed to get write node: {node_name}")
            continue

        if n["disable"].getValue():
            continue

        # nuke.execute(node_name, n.firstFrame(), n.lastFrame())
        try:
            nuke.execute(node_name, n.firstFrame(), n.lastFrame())
        except Exception:
            log.error(
                f"failed to render {node_name} with range {n.firstFrame()} - {n.lastFrame()}"
            )
            print(
                f"failed to render {node_name} with range {n.firstFrame()} - {n.lastFrame()}"
            )
            try:
                log.debug(
                    f"trying to render {node_name} for single frame {n.firstFrame()}"
                )
                print(
                    f"trying to render {node_name} for single frame {n.firstFrame()}"
                )
                nuke.execute(node_name, n.firstFrame(), n.firstFrame())
            except Exception as e:
                log.error(f"failed to execute {node_name}: {e}")
                print(f"failed to execute {node_name}: {e}")
                pass

    for node_name in new_nodes:
        nuke.delete(node_name)

    return True


def hornet_publish_configurate(data=None, nodes=None):
    """
    called from the onScriptLoad callback on the farm nuke instance,
    or from the local nuke instance when generating review media locally
    configures the nodes in the template script for this submission

    !!! on farm, this function will not have access to the full python environment !!!

    args:
        data (dict): Optionally supply data directly for debugging, in production it will come from an env variable
    """
    print("hornet_publish_configurate")

    if nodes == None:
        nodes = nuke.allNodes()

    # Initialize logger in file mode for farm rendering
    # file mode dumps a log file next to the subbission sript
    log = MiniLogger(None, file_mode=True)

    log.info(
        f"hornet_publish_configurate started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    )

    if data is None:
        d = os.environ.get("HORNET_PUBLISH", None)
        if d is None:
            log.raise_exception(
                "HORNET_PUBLISH environment variable is not set"
            )
        data = json.loads(d)

    log.debug("Input data keys: " + ", ".join(data.keys()))

    try:
        read_node = GetReadNode(log)
        log.info(f"Found read node: {read_node.name()}")
    except Exception as e:
        log.error(f"Failed to get read node: {str(e)}")
        read_node = None

    # Get file sequence information
    try:
        fs = GetFileSequence(data, log)
        log.info(
            f"File sequence: {fs.absolute_file_name} [{fs.first_frame}-{fs.last_frame}]"
        )
    except Exception as e:
        log.raise_exception(f"Failed to create file sequence: {str(e)}")

    if read_node:
        try:
            string = (
                f"{fs.absolute_file_name} {fs.first_frame}-{fs.last_frame}"
            )
            read_node["file"].fromUserText(string)
            read_node["colorspace"].setValue(data.get("colorspace", None))
            log.info(f"Configured read node: {read_node['file'].getValue()}")
        except Exception as e:
            log.error(f"Failed to configure read node: {str(e)}")

    write_nodes = [node for node in nodes if node.Class() == "Write"]
    log.info(f"Found {len(write_nodes)} write nodes")

    write_node_names = []
    for write in write_nodes:
        try:
            log.debug(f"Configuring write node: {write.name()}")
            configure_write_node(write, data, log)
            write_node_names.append(write.name())
            log.info(
                f"Configured write node: {write.name()} -> {write['file'].getValue()}"
            )
        except Exception as e:
            log.error(
                f"Failed to configure write node {write.name()}: {str(e)}"
            )

    try:
        populate_data_node(data)
        log.info(
            f"Data node populated: {data.get('shot', 'N/A')} v{data.get('version', 'N/A')}"
        )
    except Exception as e:
        log.error(f"Failed to populate data node: {str(e)}")

    # set switch nodes that determine burnin
    for n in [node for node in nodes if node.Class() == "Switch"]:
        if "burnin" in n.name():
            n["which"].setValue(data.get("burnin", 0))

    try:
        script_path = Path(nuke.toNode("root").name())
        log.debug(f"Script path: {script_path}")
    except Exception as e:
        log.error(f"Failed to get script information: {str(e)}")
        script_path = Path("unknown_script.nk")

    if write_node_names:
        write_nodes_str = "_".join(write_node_names)
        log_file_name = (
            script_path.parent / f"{script_path.stem}_{write_nodes_str}"
        ).with_suffix(".log")
    else:
        log_file_name = (script_path.parent / script_path.stem).with_suffix(
            ".log"
        )

    log.dump_log_to_file(log_file_name)

    try:
        nuke.scriptSave()
        log.info("Script saved successfully")
    except Exception as e:
        log.error(f"Failed to save script: {str(e)}")

    print(f"Log file: {log_file_name}")


def populate_data_node(data):
    data_node = nuke.toNode("Data")
    data_node["shot_name"].setValue(data["shot"])
    data_node["render_name"].setValue(data["name"])
    data_node["project_name"].setValue(data["project"])
    data_node["version"].setValue(data["version"])
    data_node["burnin"].setValue(data["burnin"])
    data_node["firstFrame"].setValue(data["first_frame"])
    data_node["lastFrame"].setValue(data["last_frame"])


def discover_write_nodes_in_script(script_path):
    current_nodes = set(nuke.allNodes())
    new_nodes = set()

    try:
        nuke.nodePaste(script_path)
        new_nodes = set(nuke.allNodes()) - current_nodes
        write_node_info = []

        for node in new_nodes:
            if node.Class() == "Write" and node["disable"].getValue() == False:
                write_node_info.append(
                    (
                        node.name(),
                        node.firstFrame(),
                        node.lastFrame(),
                    )
                )

        return write_node_info

    finally:
        for node in new_nodes:
            nuke.delete(node)


def configure_write_node(write, data, log):
    if type(write) == str:
        write = nuke.toNode(write)

    format = write["file_type"].value()
    publish_loc = Path(data["publishDir"])
    publish_loc = publish_loc / "review"
    publish_loc.mkdir(parents=True, exist_ok=True)
    if not publish_loc.exists():
        log.raise_exception("failed to create publish location")

    sanitized_colorspace_name = get_colorspace_name(
        write["colorspace"].value()
    )

    if "mov64_fps" in write.knobs():
        write["mov64_fps"].setValue(data["fps"])
        log.debug(f"fps set to: {write['mov64_fps'].getValue()}")

    new_path = f"{publish_loc.as_posix()}/{data['shot']}_{data['name']}_v{data['version']:0>3}_{write.name()}_{sanitized_colorspace_name}.{format}"
    print(f"new path: {new_path}")
    write["file"].setValue(new_path)


def build_request(submission_info, temp_script_path, publish_env_vars):
    submissionEnvVars = [
        "HORNET_ROOT",
        "NUKE_PATH",
        "OCIO",
        "OPTICAL_FLARES_PATH",
        "peregrinel_LICENSE",
        "OFX_PLUGIN_PATH",
        "RVL_SERVER",
        "neatlab_LICENSE",
    ]
    environment = dict(
        {k: os.environ[k] for k in submissionEnvVars if k in os.environ.keys()}
    )

    environment["HORNET_PUBLISH"] = publish_env_vars

    job_info = {
        # Job name, as seen in Monitor
        "Name": os.environ["AYON_PROJECT_NAME"].split("_")[0]
        + "_"
        + submission_info.get("task_name" or "task name error"),
        "UserName": getpass.getuser(),
        "Priority": int(submission_info.get("deadlinePriority")) or 95,
        "Pool": submission_info.get("deadlinePool") or "local",
        "SecondaryPool": "",
        "Group": submission_info.get("deadlineGroup") or "nuke",
        "Plugin": "Nuke",
        "Frames": submission_info.get("Frames"),
        "ChunkSize": int(submission_info.get("deadlineChunkSize", 1)) or 1,
        "LimitGroups": "nuke-limit",
        "ConcurrentTasks": int(submission_info.get("concurrentTasks", 1)),
        "BatchName": submission_info.get("batch"),
    }

    # Add dependency for frames_farm workflows (publish jobs) but not frames (local)
    deadline_job_id = submission_info.get("deadline_job_id")
    job_type = submission_info.get("job_type")
    render_target = submission_info.get("render_target")

    # Never add dependencies when render_target is "frames" (local rendering)
    if render_target == "frames":
        print("Render target is 'frames' - no dependencies will be added")
    elif deadline_job_id and job_type == "publish":
        # This is a frames_farm workflow - add publish job as dependency
        job_info["JobDependencies"] = deadline_job_id
        print(
            f"Added deadline job dependency: {deadline_job_id} (type: {job_type})"
        )
    elif job_type == "render":
        # This would be regular farm rendering - could add render job dependency here if needed
        # but user said other render targets won't be used
        pass
    # For frames (local) workflows, job_type will be None and no dependency is added

    body = {
        "JobInfo": job_info,
        "PluginInfo": {
            "SceneFile": temp_script_path.replace("\\", "/"),
            # Output directory and filename
            # "OutputFilePath": submission_info["file"].replace("\\", "/"),
            # "OutputFilePrefix": render_variables["filename_prefix"],
            # Mandatory for Deadline
            "Version": str(nuke.NUKE_VERSION_MAJOR)
            + "."
            + str(nuke.NUKE_VERSION_MINOR),
            # Resolve relative references
            "ProjectPath": nuke.script_directory().replace("\\", "/"),
            # using GPU by default
            # Only the specific write node is rendered.
            "WriteNode": submission_info.get("write_node_name"),
        },
        # Mandatory for Deadline, may be empty
        "AuxFiles": [],
    }
    job_info.update(
        {
            "EnvironmentKeyValue%d" % index: "{key}={value}".format(
                key=key, value=str(environment[key])
            )
            for index, key in enumerate(environment)
        }
    )
    return body


def GetReadNode(log):
    # TODO this could lead to collisions if there's already a node with that name for some weird reason
    read_node = nuke.toNode("PublishRead")
    if read_node is None:
        log.error("failed to get read node")
        # raise Exception("failed to get read node")
    return read_node


def GetFileSequence(data, log):
    seq_string_abs = f"sequence string absolute: {data['publishedSequence']}"
    print(f"seq_string_abs: {seq_string_abs}")
    fs = SequenceFactory.from_sequence_string_absolute(
        Path(data["publishedSequence"])
    )
    if fs is None:
        log.error("failed to get file sequence")
        # raise Exception("failed to get file sequence")
    return fs


def apply_fileseq_to_node(fs, node):
    if node.Class() != "Write":
        raise Exception(
            "Node provided to apply_fileseq_to_node is not a Write node"
        )

    node["file"].fromUserText(
        f"{fs.absolute_file_name} {fs.first_frame}-{fs.last_frame}"
    )


def copy_template_to_temp(template_path, temp_script_path):
    print(f"copy_template_to_temp: {template_path} to {temp_script_path}")
    try:
        shutil.copy(template_path, temp_script_path)
    except Exception as e:
        raise Exception(f"failed to copy template to temp: {e}")


def get_colorspace_name(colorspace):
    return COLORSPACE_LOOKUP.get(colorspace, colorspace.replace(" ", "_"))


class MiniLogger:
    def __init__(self, logger=None, file_mode=False):
        self.logger = logger
        self.file_mode = file_mode
        self.log_buffer = []

        if file_mode:
            self.log_buffer.append("=== HORNET PUBLISH LOG ===")
            self.log_buffer.append(
                f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            )

            # Add environment info only in file mode
            relevant_env_vars = [
                "AYON_WORKDIR",
                "AYON_PROJECT_NAME",
                "AYON_FOLDER_PATH",
                "NUKE_PATH",
                "OCIO",
            ]

            self.log_buffer.append("\n=== ENVIRONMENT ===")
            for env_var in relevant_env_vars:
                value = os.environ.get(env_var, "NOT SET")
                self.log_buffer.append(f"{env_var}: {value}")
            self.log_buffer.append("")

    def log(self, message, level="info"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        formatted_message = f"[{timestamp}] {level.upper()}: {message}"

        print(formatted_message)
        nuke.tprint(formatted_message)

        if self.file_mode:
            self.log_buffer.append(formatted_message)

        if self.logger:
            log_method = getattr(self.logger, level.lower(), self.logger.info)
            log_method(message)

    def info(self, message):
        """Log at info level"""
        self.log(message, "info")

    def debug(self, message):
        """Log at debug level"""
        self.log(message, "debug")

    def warning(self, message):
        """Log at warning level"""
        self.log(message, "warning")

    def error(self, message):
        """Log at error level"""
        self.log(message, "error")

    def raise_exception(self, message, exception_type=Exception):
        """Log an error message then raise an exception"""
        self.error(message)
        raise exception_type(message)

    def dump_log_to_file(self, log_file_path):
        
        if not self.file_mode:
            return

        try:
            
            with open(log_file_path, "w") as f:
                f.write("\n".join(self.log_buffer))

            self.info(f"Debug log written to: {log_file_path}")
        except Exception as e:
            self.error(f"Failed to write log file: {str(e)}")


def _get_batch_name(data):
    batch_name = data.get("jobBatchName")

    if not batch_name:
        current_file = data.get("currentFile")
        if current_file:
            batch_name = os.path.splitext(os.path.basename(current_file))[0]
        else:
            # Final fallback to project_name_version format
            batch_name = f"{data['project']}_{data['name']}_{data['version']}"

    return batch_name


def validate_template_script(template_script, logger=None):
    log = MiniLogger(logger)

    log.info(f"validating template script: {template_script}")

    if template_script is None:
        log.raise_exception(
            "template_script is None\n have you entered the correct path to the template script in the Ayon web ui?\n The setting can be found under Nuke publish plugins"
        )

    if not os.path.isfile(template_script):
        log.raise_exception(
            "template_script is not a file\n have you entered the correct path to the template script in the Ayon web ui?\n The setting can be found under Nuke publish plugins"
        )

    if os.path.splitext(template_script)[1] != ".nk":
        log.raise_exception(
            "template_script is not a .nk file\n have you entered the correct path to the template script in the Ayon web ui?\n The setting can be found under Nuke publish plugins"
        )

    return True


def resolve_submission_script(data, write_node_name, logger=None):
    log = MiniLogger(logger)

    template_script = data["template_script"]

    log.info(f"resolving submission script from: {template_script}")

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S-%f")
    script_name = os.path.splitext(os.path.basename(nuke.root().name()))[0]
    submitter_node_name = data.get("name", "__submitter_node_name_unknown__")
    representations_name = write_node_name

    name = script_name + "_" + submitter_node_name + "_" + representations_name

    submission_script = (
        "{path}/submission/publish/{name}_review_media_gen_{time}.nk".format(
            path=os.environ["AYON_WORKDIR"],
            name=name,
            time=timestamp,
        )
    )

    log.debug(f"submission script: {submission_script}")

    try:
        Path(submission_script).parent.mkdir(parents=True, exist_ok=True)
        copy_template_to_temp(template_script, submission_script)
    except Exception as e:
        log.raise_exception(f"failed to create publish temp script path: {e}")

    return submission_script

